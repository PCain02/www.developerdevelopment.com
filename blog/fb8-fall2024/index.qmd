---
author: [Pallas-Athena Cain, Coltin Colucci, Chezka Quinola, Gregory M. Kapfhammer]
title: Parsing Inputs
date: '2024-11-20'
date-format: long
categories: [post, software engineering, fuzzing book]
description: <em>How can input parsers help with fuzzing? How can parsing inputs make more inputs for fuzzing? </em>
toc: true
page-layout: full
---

## Overview

## Summary

Reminder a non-terminal is the rule or building blocks for the grammar such as `<digit>` and `<letter>` whereas a terminal is the actual digit or letter think of it as the desintation for the grammar like `2` or `a`.

Given a string you can decompose it into its constituent parts that correspond to the parts of grammar used to generate it. 

You can make a derivation tree of a given string to break it down into its grammar parts. Then it can be recombined using the same grammar to produce new strings. Trees allow us to mutatee, crossover, and recombine their parts in order to generate new valid slightly different inputs.

There are 2 main parsing classes we will look at today to make a string into a derivation tree.
- Parsing Expression Grammar Parser (PEGParser) 
- Earley Parsers

The PEGParser is efficient but limited to a specific grammar structure - rather than choosing all the rules that can potentially match it stops at the first match that succeeds.

The Earley Parser can accept any kind of context-free grammars and explore all parsing alternatives.

### To Use

To use these parsers first you have initiate a grammar.

```python
>>> from Grammars import US_PHONE_GRAMMAR
>>> us_phone_parser = EarleyParser(US_PHONE_GRAMMAR)
```

Then you use the parse method to retrieve a list of possible derivation trees:

```python
>>> trees = us_phone_parser.parse("(555)987-6543")
>>> tree = list(trees)[0]
>>> display_tree(tree)
```

![](example-tree.png)

These trees can be used for test generation. Notably for mutating and recombing existing inputs.

### Why Parsing for Fuzzing?

Sometimes trying to generate valid inputs for fuzzing can be hard. For example if you have code that can only run if the first item has to be the stings `van` or `car` to work but you are randomly generating the input the chances of getting those to output are very low.

We can give the input generator our grammar but just the grammar itself is not enough to make valid inputs. Even if you modify the fuzzer to know more about the way the inputs are formated there is still difficulty getting valid inputs.

```python
class PooledGrammarFuzzer(GrammarFuzzer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._node_cache = {}

    def update_cache(self, key, values):
        self._node_cache[key] = values

    def expand_node_randomly(self, node):
        (symbol, children) = node
        assert children is None
        if symbol in self._node_cache:
            if random.randint(0, 1) == 1:
                return super().expand_node_randomly(node)
            return copy.deepcopy(random.choice(self._node_cache[symbol]))
        return super().expand_node_randomly(node)

gf = PooledGrammarFuzzer(CSV_GRAMMAR, min_nonterminals=4)
gf.update_cache('<item>', [
    ('<item>', [('car', [])]),
    ('<item>', [('van', [])]),
])
trials = 10
time = 0
for i in range(trials):
    vehicle_info = gf.fuzz()
    try:
        print(repr(vehicle_info), end="")
        process_vehicle(vehicle_info)
    except Exception as e:
        print("\t", e)
    else:
        print()
```

```
',h,van,|'	 Invalid entry
'M,w:K,car,car,van'	 Invalid entry
'J,?Y,van,van,car,J,~D+'	 Invalid entry
'S4,car,car,o'	 invalid literal for int() with base 10: 'S4'
'2*-,van'	 not enough values to unpack (expected at least 4, got 2)
'van,%,5,]'	 Invalid entry
'van,G3{y,j,h:'	 Invalid entry
'$0;o,M,car,car'	 Invalid entry
'2d,f,e'	 not enough values to unpack (expected at least 4, got 3)
'/~NE,car,car'	 not enough values to unpack (expected at least 4, got 3)
```

**The solution: A parser. A parser can extract the template for inputs and and valid values from samples and use them for fuzzing!**

### Using a Parser

The parser processes structured input. The parsers in this chapter take an input string and turn it into a derviation tree. 

The next chapter is about using the parser for testing but this chapter is about understanding how parsers work. 

### An Ad Hoc Parser

```python
# Used to extract the information
def simple_parse_csv(mystring: str) -> DerivationTree:
    children: List[DerivationTree] = []
    tree = (START_SYMBOL, children)
    for i, line in enumerate(mystring.split('\n')):
        children.append(("record %d" % i, [(cell, [])
                                           for cell in line.split(',')]))
    return tree

# Change the orientation to Left to Right
def lr_graph(dot):
    dot.attr('node', shape='plain')
    dot.graph_attr['rankdir'] = 'LR'

tree = simple_parse_csv(mystring)
display_tree(tree, graph_attr=lr_graph)
```
![](csv-lr-graph.png)

This is a simple example and if we encounter more complexity it may not parse correctly. We can separate these incorrectly parsed inputs manually but that would be insanity! It would be so much work. Instead we use formal parsers. With those changing the external structure does not have much impact on the internal structure.

### Grammars in Parsing

Non-terminals are the building block rules that may be expanded. The symbols themselves are typically terminals unless they can be expanded further.

Here is an example derivation tree for this grammar:

```python
tree = ('<start>', [('<expr>',
                     [('<expr>', [('<integer>', [('<digit>', [('1', [])])])]),
                      ('+', []),
                      ('<expr>', [('<integer>', [('<digit>', [('2',
                                                               [])])])])])])
assert mystring == tree_to_string(tree)
display_tree(tree)
```

![](derivation-tree.png)

#### Recursion

Recursion is possible in grammar such as with `<expr>` being used in its own definition. There are two kinds of recurision to note in parsing left-recursive and right-recursive.

**Left-recursive**

A nonterminal is directly left-recursive if the left-most symbol of any of its productions is itself.

Example:

```python
LR_GRAMMAR: Grammar = {
    '<start>': ['<A>'],
    '<A>': ['<A>a', ''],
}

mystring = 'aaaaaa'
display_tree(
    ('<start>', [('<A>', [('<A>', [('<A>', []), ('a', [])]), ('a', [])]),
                 ('a', [])]))
```

![](left-tree.png)

A grammar can also be indirectly left-recursive if the left-most symbols can be expanded using their definitions to produce the nonterminal as the left-most symbol of the expansion. For example `<integer>` will be considered hidden-left recursive if `<digit>` could derive an empty string.

**Right-recurisve**

Right recursive is the same idea but it expands the other direction.

```python
RR_GRAMMAR: Grammar = {
    '<start>': ['<A>'],
    '<A>': ['a<A>', ''],
}

display_tree(('<start>', [('<A>', [
                  ('a', []), ('<A>', [('a', []), ('<A>', [('a', []), ('<A>', [])])])])]
             ))
```

![](right-tree.png)

#### Abiguity

Sometimes there are multiple trees from the same grammar called **parses**. The sting `1+2+3` has two ways it can be parsed using the `A1_GRAMMAR`. One method to deal with this is to simply choose an order of resolution, perhaps choose the first tree in any case (PEGParser). Another approach returns all the trees (Earley parser).

### A Parser Class

To develop parsers there needs to be defined interface for parsing. There are two approaches to parsing a string using a grammar.

The first approach is to use a lexer.  A lexer tokenizes an incoming string then feeds the grammar one token at a time. Each token represents a meaningful unit such as a number or keyword. The lexer handles the tokenization process, while the parser only has to focus on understanding the structure of the language using these tokens. The result of parsing is usually a shallow derivation tree, which can be converted into an Abstract Syntax Tree (AST).

The second approach is using a tree pruner. This occurs after the complete parse. The tree pruner removes the nodes that correspond to individual tokens and then replaces them with their actual string values as leaf nodes. This approach is more flexible and powerful additionally there is no separate step for lexing and parsing.

### Parsing Expression Grammars

Parsing Expression Grammars (PEG) are recognition based grammars that specify a sequence of steps to take to parse a given string. Parsing expression grammars are represented by a set of nonterminals and corresponding alternatives representing how to match each. 

```python
PEG1 = {
    '<start>': ['a', 'b']
}
```

Unlike context-free grammars alternatives represent ordered choices, meaning the parser will stop at the first matching rule it finds. 

```python
PEG2 = {
    '<start>': ['ab', 'abc']
}
```

### The Packrat Parser for Predicate Expression Grammars

Packrat parsing is one of the simplest parsing techniques used for PEGs. It works by caching results from previous parsing steps so if the same problem comes up again it can reuse the cached solution instead of redoing the work. 

```python
mystring = "1 + (2 * 3)"
peg = PEGParser(EXPR_GRAMMAR)
for tree in peg.parse(mystring):
    assert tree_to_string(tree) == mystring
    display(display_tree(tree))
```

![](Packrat.png)

### Parsing Context-Free Grammars

#### Problems with PEG

PEGs may at first appear simple but in some cases may include a little bit more thinking.

```python
PEG_SURPRISE: Grammar = {
    "<A>": ["a<A>a", "aa"]
}
```
When interpreted as a context free grammar and used as a string generator it will produce strings of the from aa, aaaa, aaaaaa. it produces strings where the number of a is 2 * n.

PEGs are oriented towards language recognition and it is also unclear how to translate a PEG to a CFG. With the main focusing being fuzzing next we are going to look at parsers tha can accept context-free grammars. 

# Earley Parsers

The **Earley parser** is a versatile general-purpose parser capable of parsing any arbitrary Context-Free Grammar (CFG). It was developed by **Jay Earley** in 1970 for computational linguistics. Although its computational complexity is \(O(n^3)\) for parsing strings with arbitrary grammars, it can efficiently parse strings with unambiguous grammars in \(O(n^2)\) time. It can also parse all **LR(k)** grammars in linear time, \(O(n)\) as noted by **Joop M.I.M. Leo** in 1991. Further enhancements, particularly in handling epsilon rules, were introduced by **Aycock et al.** in 2002.

**Discussion Question**: How does the computational complexity of the Earley parser affect its efficiency when dealing with large or ambiguous grammars? Can the parser be optimized further for specific use cases?

### Grammar Restrictions

A notable limitation in the Earley parser implementation is that the start symbol can only have one alternative in its alternative expressions. However, this restriction can be easily bypassed by introducing a new start symbol. For example, given a grammar:

```python
grammar = {
    '<start>': ['<A>', '<B>'],
    ...
}
```

You can rewrite it to conform to the single-alternative rule:

```python
grammar = {
    '<start>': ['<start_>'],
    '<start_>': ['<A>', '<B>'],
    ...
}
```

**Discussion Question**: Why might the Earley parser require the start symbol to have only one alternative, and what implications does this have for the flexibility of grammar representations?

### Implementing the EarleyParser

To implement the **EarleyParser**, we can create a class derived from a basic `Parser` class. Here's an example of how to use the Earley parser with some simple expressions:

### Example 1: Parsing an Expression

```python
mystring = "1 + (2 * 3)"
earley = EarleyParser(EXPR_GRAMMAR)
for tree in earley.parse(mystring):
    assert tree_to_string(tree) == mystring
    display(display_tree(tree))
```

The derivation tree for this example might look like this:

```
<start>
  <expr>
    <term>
      +
      <expr>
        <factor>
          <integer>
            <digit>
              1 (49)
        <term>
          <factor>
            ( (40)
        <expr>
          ) (41)
        <term>
          <factor>
            *
            <term>
              <integer>
                <digit>
                  2 (50)
              <factor>
                <integer>
                  <digit>
                    3 (51)
```

### Example 2: Parsing Another Expression with Decimals

```python
mystring = "1 * (2 + 3.35)"
for tree in earley.parse(mystring):
    assert tree_to_string(tree) == mystring
    display(display_tree(tree))
```

The derivation tree here might look like:

```
<start>
  <expr>
    <term>
      <factor>
        *
        <term>
          <integer>
            <digit>
              1 (49)
          <factor>
            ( (40)
        <expr>
          ) (41)
        <term>
          +
        <expr>
          <factor>
            <integer>
              <digit>
                2 (50)
          <term>
            <factor>
              <integer>
                . (46)
              <integer>
                <digit>
                  3 (51)
```

**Discussion Question**: How does the parser handle nested expressions, and how does the structure of the derivation tree help us understand the parsing process?

### Excursion: Testing the Parsers

Testing parsers can be done effectively by generating random grammars and their corresponding strings. This provides an interesting way to ensure that parsers are working as expected.

### Grammar Generation Code Example

```python
def prod_line_grammar(nonterminals, terminals):
    g = {
        '<start>': ['<symbols>'],
        '<symbols>': ['<symbol><symbols>', '<symbol>'],
        '<symbol>': ['<nonterminals>', '<terminals>'],
        '<nonterminals>': ['<lt><alpha><gt>'],
        '<lt>': ['<'],
        '<gt>': ['>'],
        '<alpha>': nonterminals,
        '<terminals>': terminals
    }

    if not nonterminals:
        g['<nonterminals>'] = ['']
        del g['<lt>']
        del g['<alpha>']
        del g['<gt>']

    return g
```

### Making Rules for Non-Terminals and Terminals

```python
def make_rule(nonterminals, terminals, num_alts):
    prod_grammar = prod_line_grammar(nonterminals, terminals)

    gf = GrammarFuzzer(prod_grammar, min_nonterminals=3, max_nonterminals=5)
    name = "<%s>" % ''.join(random.choices(string.ascii_uppercase, k=3))

    return (name, [gf.fuzz() for _ in range(num_alts)])
```

### Example Grammar Fuzzing

```python
make_rule(["A", "B", "C"], ["1", "2", "3"], 3)
('<FYU>', ['<C>23', '<C><A>', '<B><C>3'])
```

### Creating a Complete Grammar

```python
def make_grammar(num_symbols=3, num_alts=3):
    terminals = list(string.ascii_lowercase)
    grammar = {}
    name = None
    for _ in range(num_symbols):
        nonterminals = [k[1:-1] for k in grammar.keys()]
        name, expansions = make_rule(nonterminals, terminals, num_alts)
        grammar[name] = expansions

    grammar[START_SYMBOL] = [name]

    for nonterminal in unreachable_nonterminals(grammar):
        del grammar[nonterminal]

    assert is_valid_grammar(grammar)

    return grammar
```

### Using the Grammar with EarleyParser

Once the grammar is generated, you can use the Earley parser to parse strings produced by the grammar:

```python
for i in range(5):
    my_grammar = make_grammar()
    print(my_grammar)
    parser = EarleyParser(my_grammar)
    mygf = GrammarFuzzer(my_grammar)
    s = mygf.fuzz()
    print(s)
    for tree in parser.parse(s):
        assert tree_to_string(tree) == s
        display_tree(tree)
```

**Discussion Question**: What challenges might arise when testing parsers with random grammars, and how can fuzzing help identify edge cases or parsing issues?

### Background

There are various parsing techniques available, each designed to handle specific types of grammars. Some common approaches include **LL** and **LR** parsing. **LL parsing** processes the input left-to-right, using a leftmost derivation. **LR parsing** works similarly but uses a rightmost derivation. These approaches are particularly efficient, with **LR(1)** grammars parseable in linear time.

**Discussion Question**: How do LL and LR parsing differ in terms of efficiency and applicability, and when might one be preferred over the other?

### Complexities of Parsing

For **arbitrary CFGs**, the best-known algorithms typically have a worst-case complexity of \(O(n^3)\). More advanced techniques, like **boolean matrix multiplication**, could theoretically reduce this further, but practical implementations often remain cubic.

**Discussion Question**: What factors influence the complexity of parsing algorithms, and how can they be optimized for practical use cases?

### Peg Parsing

**Parsing Expression Grammars (PEGs)** are another important concept. While PEGs are effective for many contexts, their ability to represent certain languages (like **anbncn**) remains an open question. However, PEGs have the advantage of providing a simple top-down interpretation, making them intuitive for writing parsers.

**Discussion Question**: In what scenarios would PEG parsing be more advantageous than other parsing techniques, and what are its limitations?

This concludes our overview of the Earley parser and its role in parsing arbitrary context-free grammars. By integrating it with fuzzing techniques, we can effectively test parsers against a variety of grammars, ensuring robustness and correctness.

## Reflection
The article "Parsing Inputs" highlights the critical role of parsers in fuzzing by emphasizing how input parsers break down strings into derivation trees, which can then be mutated and recombined to generate new, valid inputs. I found the explanation of non-terminals, terminals, and the use of parsers like PEGParser and EarleyParser to be insightful. The examples, particularly the one about using a parser to improve fuzzing, demonstrated how incorporating grammar into the fuzzing process can significantly increase the likelihood of generating valid inputs, thus making fuzzing more effective.

I particularly appreciated the in-depth exploration of the EarleyParser and its flexibility in handling context-free grammars, which was crucial for understanding how such parsers can be used for fuzz testing. The emphasis on using parsers to simplify complex input generation is an actionable takeaway that I will apply when generating more structured inputs in future fuzzing exercises. The discussion of recursion, ambiguity, and grammar restrictions also gave me a clearer understanding of the complexities involved in parsing different types of grammars, which can be beneficial when working with more complex systems.

## Action Items
The action items derived from the reading emphasize the importance of understanding and implementing parsing techniques for enhancing fuzzing capabilities. To start, it is crucial to experiment with different parsing methods, such as PEG and Earley Parsers, to determine their applicability in generating valid input for fuzzing tools. The next step is to implement a parser class with the ability to handle context-free grammars, as it can support more complex structures and enhance the flexibility of fuzzing processes. Furthermore, the implementation of a grammar fuzzer should be explored, particularly using techniques like cache updates and random node expansions, to facilitate the generation of diverse test cases. Additionally, it is important to apply these parsing methods to generate meaningful and valid test data, as well as to test the parsers against various types of grammars, ensuring that all edge cases are addressed. Finally, an evaluation of the performance of each parsing method in terms of parsing efficiency and the quality of generated test inputs should be conducted, with the goal of optimizing fuzzing strategies.

<!-- Include the license statement for the online book -->
{{< include /_fuzzingbook-reference.qmd >}}

<!-- Include reference back to the listing of blog posts -->
{{< include /_back-blog.qmd >}}
